Дедлайн: 20 октября, 23.59.

Ноутбуки сохранять с названием в формате SurnameTask2.ipynb

Перед выполнением задания --- укажите себя в исполнителях, чтобы не было дублей.


# Оформление задания
Задание сдается в jupyter notebook. Структура:
* Заголовок ноутбука
* Описание задания
* Код
* Визуализация
* Краткий комментарий, выводы

Задание оценивается по критериям:
* Корректность
* Наглядность
* Адекватность кода
* Оформление результатов


# Список заданий
1. [Исполнитель: TODO] Визуализировать распределения Кента в зависимости от его параметров. 
* [wiki](https://en.wikipedia.org/wiki/Kent_distribution)

2. [Исполнитель: TODO] Визуализировать матрицу ковариации, а также апостериорную вероятность в окрестности точки оптимума. Выборка: один из стандартных датасетов sklearn. Модель: логистическая регрессия. Априорное распределение параметров: нормальное распределение с положительно-определеннной матрицей ковариации (N(0, A^{-1})). Оптимизацию матрицы провести итеративным методом с использованием аппроксимации Лапласа. 
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [оптимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)


3. [Исполнитель: TODO] Визуализировать матрицы ковариаций для каждого экстремума многоэкстремальной задачи классификации. Визуализацию проводить с помощью интерактивных графиков.  Модель: однослойная нейронной сеть. Выборка: произвольная, содержащая несколько экстремумов (допускается использование синтетических выборок). Априорное распределение параметров: нормальное распределение с положительно-определеннной матрицей ковариации (N(0, A^{-1})). Оптимизацию матрицы провести итеративным методом с использованием аппроксимации Лапласа. 
* [оптимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)

4. [Исполнитель: TODO] Визуализировать сходимость апостериорного распределения к априорному в зависимости от количества итераций оптимизации гиперпараметров.  Модель: произвольная. Выборка: произвольная (допускается использование синтетических выборок).  Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)). Оптимизацию sigma провести итеративным методом с использованием аппроксимации Лапласа. 
* [опиимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)

5. [Исполнитель: TODO]  Визуализировать эмпирическое распределение параметров модели в зависимости от значения sigma^2.  Модель: произвольная. Выборка: произвольная.  Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)).

6. [Исполнитель: TODO]  Визуализировать эмпирическое распределение параметров модели в зависимости от значения $p$ для модели с L^p регуляризацией.  Модель: произвольная. Выборка: одна из стандартных датасетов sklearn. 
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)

7. [Исполнитель: TODO]  Визуализировать эмпирическое распределение параметров модели однослойной нейросети в зависимости от количества нейронов на скрытом слое (рассматривать в том числе и сильно переусложненные модели). Модель: однослойная нейросеть. Выборка: MNIST или схожая.


8. [Исполнитель: TODO]  Визуализировать MDL, аналогично слайду 7 из презентации, в зависимости от размера скрытого слоя однослойной нейросети нейронной сети. Модель: однослойная нейросеть. Выборка: одна из стандартных датасетов sklearn.  Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)).  Оптимизацию sigma провести итеративным методом с использованием аппрксимации Лапласа. 
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [оптимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)
* [презентация](https://github.com/Intelligent-Systems-Phystech/BMM-21/blob/master/slides/slides_3_mdl.pdf)
* [источник рисунка в презентации](https://www.inference.org.uk/itprnn/book.pdf)


9. [Исполнитель: TODO]  Рассмотреть логистическую регрессию с двумя параметрами. Построить поверхность со следующими осями: w1, w2, sigma (оптимальный параметр априорного распределения для модели с параметрами [w1, w2]). Цветом поверхности задать значение апостериорной вероятности в точки поверхности. Модель: логистическая регрессия. Выборка: произвольная (допускается использование синтетических выборок или использование двух наиболее значимых параметров на одном из стандартных датасетов sklearn).  Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)). Оптимальное значение sigma выбрать перебором.
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)

10. [Исполнитель: TODO]  Построить интерактивный график зависимости апостериорного распределения параметров от количества параметров модели, sigma (оптимальный параметр априорного распределения для модели), эмпирической дисперсии параметров. Для каждого значения количества параметров в интерактивном графике построить поверхность с осями: sigma^2, эмпирическая дисперсия параметров, апостериорная вероятность параметров. Цвет каждой поверхности задать зависимым от Evidence. Модель: логистическая регрессия на полиномах различных степеней от одного признака.  Выборка: произвольная (допускается использование синтетических выборок или использование одного наиболее значимого параметра на одном из стандартных датасетов sklearn).  Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)).
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
  
  
11. [Исполнитель: TODO] Повторить график выбора модели по Evidence с реальными экспериментами. График взять из презентации (слайд 15, "хема выбора модели").
12. Выборка: произвольная (допускается использование синтетических выборок или использование одного наиболее значимого параметра на одном из стандартных датасетов sklearn). Модель: набор моделей, соответствующих полиномаильной регрессии (сложность модели будет соответствовать степени полинома) или однослойная нейросеть (сложность будет соответствовать количеству нейронов на скрытом слое). Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)).   Оптимизацию sigma провести итеративным методом с использованием аппроксимации Лапласа. 
* [презентация](https://github.com/Intelligent-Systems-Phystech/BMM-21/blob/master/slides/slides_2_inference.pdf)
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [оптимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)

