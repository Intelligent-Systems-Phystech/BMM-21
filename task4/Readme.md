Дедлайн: 15 декабря, 23.59.

Ноутбуки сохранять с названием в формате SurnameTask4.ipynb

**Важно**: пожалуйста, напишите на bakhteev@phystech.edu, если задание будет готово до дедлайна.

Перед выполнением задания --- укажите себя в исполнителях, чтобы не было дублей.


# Оформление задания
Задание сдается в jupyter notebook. Структура:
* Заголовок ноутбука
* Описание задания
* Код
* Визуализация
* Краткий комментарий, выводы

Задание оценивается по критериям:
* Корректность
* Наглядность
* Адекватность кода
* Оформление результатов
* Перекрестная защита результатов


# Список заданий

1. [Исполнитель: Шокоров] Повторить график 28.6 из книги MacKay. Выборка: одна из стандартных датасетов sklearn. Модели: несколько моделей линейной регрессии:
    1. соответствующая оптимальному значению Evidence, с оптимальными значениями гиперпараметров
    2. с сниженной дисперсией sigma^2 относительно оптимальных значений
    3. со смещением среднего значения параметров относительно оптимальных значений (см. график)
Априорное распределение параметров: нормальное распределение со скалярным параметром дисперсии дисперсией (N(0, sigma^2 * I)).
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [книга](http://www.inference.org.uk/itprnn/book.pdf)
* [оптимизация гиперпараметров](http://strijov.com/papers/HyperOptimizationEng.pdf)


2. [Исполнитель: Бишук] Нарисовать зависимость качества классификации от разных видов Dataset Shift. Визуализация (несколькими графиками) должна показывать как меняется выборка в зависимости от значимости сдвига, как меняется качество классификации в зависимости от значиомсти сдвига. Выборки: несколько синтетических 2d выборок. 
* [статья](https://rtg.cis.upenn.edu/cis700-2019/papers/dataset-shift/dataset-shift-terminology.pdf)


3. [Исполнитель: Панкратов] Визуализировать зависимость дисперсии вариационного распределения от номера слоя в многослойной нейросети. Построить визуализацию для разных априорных распределений: N(0, 10^{-2}), N(0, 10^{-1}), N(0, 1), N(0, 10), N(0, 100).
Выборка: MNIST, Fashion-MNIST, CIFAR-10 или любая другая, используемая в DL. Модель: многослойная нейросеть, не менее 3 слоев. Вариационное распределение: нормально с диагональной матрицей ковариаций.
* [вар. вывод](https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)
* [полезный репозиторий с различными реализациями байесовских сетей](https://github.com/JavierAntoran/Bayesian-Neural-Networks#uncertainty-decomposition)

4. [Исполнитель: Сафиуллин] Визуализировать KL(q|p), KL(p|q), 0.5 KL(q|p) + 0.5 KL (p|q) для вариационного вывода в зависимости от количества итераций оптимизации.  Выборка: MNIST, Fashion-MNIST, CIFAR-10 или любая другая, используемая в DL. Модель: произвольная нейросеть. Вариационное распределение: нормальное с диагональной матрицей ковариаций.
* [вар. вывод](https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)
* [полезный репозиторий с различными реализациями байесовских сетей](https://github.com/JavierAntoran/Bayesian-Neural-Networks#uncertainty-decomposition)

5. [Исполнитель: Кора] Провести оптимизацию следующими методами: с использованием Evidence с аппроксимацией Лапласа, с использованием ELBO и наивной оптимизацией (смотри Graves 2011), с использованием ELBO и одновременной оптимизацией параметров и гиперпараметров (в одной процедуре одноуровневой оптимизации). Сравнить результаты (ожидается анализ, текст), а также визуализировать расстояние (0.5 KL(p1|p2) + 0.5 KL (p2|p1)) между полученными априорными распределениями в зависимости от количества итераций оптимизации.  Априорное распределение: нормальное с диагональной матрицей ковариаций. Оптимизируемые гиперпараметры: среднее и ковариация. 
Выборка: одна из стандартных датасетов sklearn. Модель: логистическая регрессия.
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)
* [аппроксимация Лапласа](http://strijov.com/papers/HyperOptimizationEng.pdf)
* [вар. вывод](https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)

6. [Исполнитель: Филатов]  Для  l1+l2 регуляризации визуализировать зависимость функции потерь на контроле и обучении от значений коэффициентов регуляризации. Координаты: коэффициенты перед l1-регуляризацией, l2-регуляризацией, потеря на обучении. Цвет поверхности - потеря на контроле.
Выборка: одна из стандартных датасетов sklearn. Модель: линейная или логистическая регрессия. 
* [datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)

7. [Исполнитель: Гребенькова] Повторить эксперимент по удалению параметров из статьи Graves (график 4).  Выборка: MNIST, Fashion-MNIST, CIFAR-10 или любая другая, используемая в DL. Модель: произвольная нейросеть. Вариационное распределение: нормальное с диагональной матрицей ковариаций.
* [вар. вывод](https://papers.nips.cc/paper/2011/file/7eb3c8be3d411e8ebfab08eba5f49632-Paper.pdf)
* [полезный репозиторий с различными реализациями байесовских сетей](https://github.com/JavierAntoran/Bayesian-Neural-Networks#uncertainty-decomposition)

8. [Исполнитель: TODO] Визуализировать (интерактивным графиком или анимацией) сходимость распределения invertible gaussian reparametrization. Провести оптимизацию вида KL(q|p) -> min, где q и p принадлежат одному семейству, параметры распределений должны сэмплироваться случайно при каждом запуске визуализации.
График: симплекс (треугольник) с изменением плотности в зависимости от шага оптимизации.
* [IGR](https://arxiv.org/pdf/1912.09588.pdf)
* [Пример визуализации плотности на симплексе](http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/)

